var tipuesearch = {"pages":[{"title":"Understanding Confidence Interval with Illustrations","text":"Comprehensive Confidence Intervals for Python Developers Variantions of different confidence intervals, their assumptions, strength and weakness, when to use, and when not to use. Confidence interval is uncertainty in summary statistic represented as a range. In the other words, it is a range of values we are fairly sure our true value lies in. For example: I am 95% confident that the population mean falls between 8.76 and 15.88 $\\rightarrow$ (12.32 $\\pm$ 3.56) Confidence interval tells you how confident you can be that the results from a poll or survey reflect what you would expect to find if it were possible to survey the entire population. It is difficult to obtain measurement data of an entire data set ( population ) due to limited resource & time. Your best shot is to survey a small fraction ( samples ) of the entire data set, and pray that your sample data represents the population reasonably well. Sample data may not be a good representation of a population by numerous factors (Ex: bias), and as a result, uncertainty is always introduced in any estimations derived from sample data. Due to the uncertainty involved with sample data, any statistical estimation needs to be delivered in a range, not in a point estimate . How well a sample statistic estimates an underlying population parameter is always an issue ( Population vs. Samples ). A confidence interval addresses this issue by providing a range of values, which is likely to contain the population parameter of interest. Contents 1 Understanding confidence interval with analogy Example 1: Uncertainty in rock porosity Example 2: Purity of methamphetamine (crystal) in Breaking Bad 2 Key takeaways 3 Population vs Samples Notes: Population variance ($\\sigma&#94;2$) vs. Sample variance ($s&#94;2$) Pythonic Tip: Difference between Numpy variance and Pandas variance 4 Confidence interval of normal distribution 4.1 Confidence interval of mean Notes: Distribution of various statistics Notes: z-score vs t-score Pythonic Tip: Computing confidence interval of mean with SciPy 4.2 Confidence interval of difference in mean Notes: Comparing means of more than two samples with ANOVA 4.2.1 Independent (unpaired) samples, equal variance - Student's t-interval Pythonic Tip: Computing student's t-interval 4.2.2 Independent (unpaired) samples, unequal variance - Welch's t-interval Pythonic Tip: Computing Welch's t-interval 4.2.3 Dependent (paired) samples - Paired t-interval Pythonic Tip: Computing paired t-interval Notes: Deciding which t-test to use 4.3 Confidence interval of variance 4.4 Confidence interval of other statistics: Bootstrap 4.5 Estimating sample size needed 4.6 Two-tailed vs one-tailed interval Notes: Robustness of confidence interval to non-normality 5 Confidence interval of non-normal distribution 5.1 Comparing central tendency of populations 5.1.1 Confidence interval of median: Mann-Whitney U test 5.2 Credible interval 5.3 Transform to normal distribution with Box-Cox 5.4 Bootstrapping 5.4.1 Bootstrap CI of statistics 5.4.1 Bootstrap CI of difference in statistics 6 FAQ's 6.1 What is confidence level? 6.2 What is significance level? 6.3 What is margin of error? 6.4 What is degrees of freedom? 6.5 Non-normality and outliers 6.6 kurtosis 7 Worked Python examples 1. Understanding confidence interval with analogy If you've taken a science class with lab reports in your highschool or college, you probably had to include measurement error in your lab reports. For example, if you were asked to measure the length of a paper clip with a ruler, you have to include $\\pm0.5 \\,\\text{cm}$ or $\\pm0.05\\,\\text{cm}$ (depending on the spacing of tick marks) to account for the measurement error that shows the precision of your measuring tool. Based on figure (1) , the paper clip seems to be about 2.7 cm long, but we don't know for sure because the tickmarks in the ruler is not precise enough to measure decimal length. However, I can tell with 100% confidence that the paper clip has a length between 2 ~ 3 cm, because the clip is between 2cm and 3cm tickmarks. You record the length of the paper clip in a range , instead of a point estimate , to account for the uncertainty introduced by the limitation of the measuring tool. Figure 1: Measurement error in ruler Similar idea can be applied to a confidence interval of mean . You want to obtain a mean of a whole data set ( population ), but you can measure values of only a small fraction ( samples ) of the whole data set. This boils down to the traditional issue of Sample vs Population , due to the cost of obtaining measurement data of a large data set. Uncertainty is introduced in your samples, because you don't know if your samples are 100% representative of the population, free of bias. Therefore, you deliver your conclusion in a range, not in a point estimate, to account for the uncertainty. Example 1: Uncertainty in rock porosity A reservoir engineer in the oil & gas industry wants to know the rock porosity of a formation to estimate the total oil reserve 9,500 ft underground. Due to the high cost of obtaining rock core samples from the deep formations, he could acquire only 12 rock core samples. Since the uncertainty of a point estimation scales inversely with a sample size, his estimation is subject to non-negligible uncertainty. He obtains 14.5% average rock porosity with 4.3% standard deviation. Executives in the company wants to know the worst-case scenario and the best-case scenario to make business decisions. You can convey your estimation of average porosity with uncertainty by constructing the confidence interval of mean . Assuming that you have a reason to believe that the rock porosity follows normal distribution, you can construct its 80% confidence interval, with the procedure described below : In [133]: stats . t . interval ( 1 - 0.2 , 12 - 1 , loc = 14.5 , scale = 4.3 / np . sqrt ( 12 )) Out[133]: (12.807569748569543, 16.19243025143046) In the worst-case scenario, the rock formation at 9,500 ft underground has 12.8% porosity. In the best-case scenario, the oil reservoir has 16.2% porosity. The same procedures can be applied for the core samples collected at different depths, which give us the confidence interval plot of rock porosities shown in figure (2) . Figure 2: Confidence interval of core samples porosities along depths Source Code For Figure (2) import numpy as np from scipy import stats import matplotlib.pyplot as plt np.random.seed(39) depth = [i * 10 + 8000 for i in range(100)] l = len(depth) avg_por = [] p10_por = [] p90_por = [] for i, item in enumerate(depth): # You collect 12 rock core samples for each depth # Assume that sample porosity follows a normal distribution sample_size = 12 por_samples = np.random.normal(loc=0.15 - i/2000, scale=0.022, size=sample_size) avg_por.append(np.mean(por_samples)) p10, p90 = stats.t.interval(1 - 0.2, sample_size - 1, loc=np.mean(por_samples), scale=stats.sem(por_samples)) p10_por.append(p10) p90_por.append(p90) plt.style.use('seaborn-whitegrid') fig, ax = plt.subplots(1, 2, figsize=(8, 4)) ax[0].plot(avg_por[:l//2], depth[:l//2], 'k', label='P50', alpha=0.8) ax[0].plot(p10_por[:l//2], depth[:l//2], 'grey', linewidth=0.7, label='P10', linestyle='--') ax[0].plot(p90_por[:l//2], depth[:l//2], 'grey', linewidth=0.7, label='P90') ax[0].set_xlim(0.08, 0.17) ax[0].set_ylabel('Depth (ft)', fontsize=15) ax[0].set_xlabel('Porosity', fontsize=15) ax[0].fill_betweenx(depth[:l//2], p10_por[:l//2], p90_por[:l//2], facecolor='lightgrey', alpha=0.3) ax[0].invert_yaxis() ax[1].plot(avg_por[l//2:], depth[l//2:], 'k', label='P50', alpha=0.8) ax[1].plot(p10_por[l//2:], depth[l//2:], 'grey', linewidth=0.7, label='P10', linestyle='--') ax[1].plot(p90_por[l//2:], depth[l//2:], 'grey', linewidth=0.7, label='P90') ax[1].set_xlim(0.08, 0.17) ax[1].set_xlabel('Porosity', fontsize=15) ax[1].legend(loc='best', fontsize=14, framealpha=1, frameon=True) ax[1].fill_betweenx(depth[l//2:], p10_por[l//2:], p90_por[l//2:], facecolor='lightgrey', alpha=0.3) ax[1].invert_yaxis() Example 2: Purity of methamphetamine (crystal) in Breaking Bad 21 batches of crystal cooked by Mr. White shows 99.1% average purity with 3% standard deviation. 18 batches of crystal cooked by Mr. Pinkman shows 96.2% average purity with 4% standard deviation. Does Mr. White always cook better crystal than Mr. Pinkman, or is it possible for Mr. Pinkman to beat Mr. White in purity of cooked crystals, by luck? We can construct 95% confidence interval assuming normal distribution, with the procedure described below : In [78]: # Mr. White's stats . t . interval ( 1 - 0.05 , 21 - 1 , loc = 99.1 , scale = 3 / np . sqrt ( 21 )) Out[78]: (97.73441637228476, 100.46558362771523) In [79]: # Mr. Pinkman's stats . t . interval ( 1 - 0.05 , 18 - 1 , loc = 96.2 , scale = 4 / np . sqrt ( 18 )) Out[79]: (94.21084679714819, 98.18915320285181) There's a small overlap between the confidence intervals of Mr. White's and Mr. Pinkman's. Although it is true that Mr. White is a better cooker, Mr. Pinkman can cook a purer batch of crystals by a small chance, if he has the luck. Comparing the means of two sample data sets is closely related to constructing confidence interval of difference in mean . Figure 3: Overlap in the 95% confidence interval of two samples Source Code For Figure (3) import matplotlib.pyplot as plt from scipy import stats import numpy as np conf_pinkman = stats.t.interval(1 - 0.05, 18 - 1, loc=96.2, scale= 4 / np.sqrt(18)) conf_white = stats.t.interval(1 - 0.05, 21 - 1, loc=99.1, scale= 3 / np.sqrt(21)) plt.style.use('seaborn-whitegrid') fig, ax = plt.subplots(figsize=(5, 2)) ax.errorbar(99.1, 1, xerr=(conf_white[1] - conf_white[0]) / 2, fmt='o', markersize=8, capsize=5, label='Mr. White\\'s', color='grey') ax.errorbar(96.2, 0, xerr=(conf_pinkman[1] - conf_pinkman[0]) / 2, fmt='o', markersize=8, capsize=5, label='Mr. Pinkman\\'s', color='k') ax.set_ylim(-0.6, 1.6) ax.fill_betweenx([1, 0], conf_white[0], conf_pinkman[1], facecolor='lightgrey', alpha=0.3) ax.legend(loc='best', fontsize=11, framealpha=1, frameon=True) ax.set_xlabel('Purity (%)', fontsize=12) ax.yaxis.set_major_formatter(plt.NullFormatter()) fig.tight_layout(); 2. Key takeaways 1. Confidence interval quantifies uncertainty of statistical estimation Confidence interval qunatifies the uncertainty related to a statistical estimation to mitigate the issue of Population vs. Samples . It is always expressed in a range like — $\\text{C.I.}: \\quad x \\pm 3.43$ or $-51.4 < x < -43.2$ 2. Confidence interval is the basis of parametric hypothesis tests Confidence interval is the basis of parametric hypothesis tests. For example, t-test computes its p-value using the confidence interval of difference in mean . When samples follow a normal distribution, and therefore their centeral tendency can be described by their means, t-test can be used to conclude if two distributions are significantly different from each other. 3. Formula for confidence interval varies with statistics For confidence interval of mean $$ \\text{C.I.}_{\\text{mean}}: \\quad \\mu \\pm (t_{\\frac{\\alpha}{2},df} \\times \\frac{s}{\\sqrt{n}})$$ For confidence interval of difference in mean $$ \\text{C.I.}_{\\Delta \\text{mean}}: \\quad (\\mu_{1}- \\mu_{2}) \\pm (t_{1-\\frac{\\alpha}{2},df} \\times \\sqrt{\\frac{s_1&#94;2}{n_1}+\\frac{s_2&#94;2}{n_2}})$$ For confidence interval of proportion $$ \\text{C.I.}_{\\text{proportion}}: \\quad \\hat{p} \\pm (t_{\\frac{\\alpha}{2},df} \\times \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} )$$ For confidence interval of variance $$ \\text{C.I.}_{\\text{variance}}: \\frac{(n-1)s&#94;{2}}{\\chi&#94;{2}_{\\frac{\\alpha}{2}}} \\leq \\sigma&#94;2 \\leq \\frac{(n-1)s&#94;{2}}{\\chi&#94;{2}_{1-\\frac{\\alpha}{2}}}$$ For confidence interval of standard deviation $$ \\text{C.I.}_{\\text{standard deviation}}: \\sqrt{\\frac{(n-1)s&#94;{2}}{\\chi&#94;{2}_{\\frac{\\alpha}{2}}}} \\leq \\sigma \\leq \\sqrt{\\frac{(n-1)s&#94;{2}}{\\chi&#94;{2}_{1-\\frac{\\alpha}{2}}}}$$ Different analytical solutions exist for different statistics. However, confidence interval for many other statistics cannot be analytically solved, simply because there are no formulas for them. If the statistic of your interest does not have an analytical solution for its confidence interval, or you simply don't know it, numerical methods like boostrapping can be a good alternative (and its powerful). 4. Things are VERY different if sample data set is not normally distributed The equations listed above are not valid if sample data set is not normally distributed . In case of non-normally distributed data, its confidence interval can be obatined with non-parametric methods like boostrapping , or instead use credible interval , which is a Baysian equivalent of confidence interval. Or you can transform your data into normal distribution using Box-Cox transformation . 5. 95% C.I. does not mean 95% of the sample data lie within the interval. It means that there's 95% chance that the estimated statistic falls within the interval. 95% confidence interval relates to the reliability of the estimation procedure. Ex: How reliable is your estimation of population variance? 6. Always use t-score instead of z-score When constructing confidence interval of mean, or running t-test, always use t-score instead of z-score. This is described in detail below . 7. Bigger sample size gives narrower confidence intervals Intuitively, this is because the more samples we have, the less uncertainty we have with our statistical estimation. Mathematically, this is because the the confidence interval is inversely related to the sample size $n$, as shown in eq (1) . 8. Means are not always equivalent to central tendency When samples are not normally distributed, their means are not a good measure of their centeral tendencies . For example, if you are comparing the means of two non-normal data sets with t-test to conclude if they came from the same population, your approach is wrong. The more viable alternative would be to use non-parametric alternatives that uses median, or other statistics that capture the central tendency of non-normal distributions. 3. Population vs. samples Confidence interval describes the amount of uncertainty associated with a sample estimate of a population parameter. One needs to have a good understanding of the difference between samples and population to understand the necessity of delivering statistical estimations in a range, a.k.a. confidence interval. Figure 1: Population vs samples Population : data set that contains all members of a specified group. Ex: ALL people living in the US. Samples : data set that contains a part, or a subset, of a population Ex: SOME people living in the US. Let's say that you are conducting a phone-call survey to investigate the society's perception of The Affordable Care Act (\"Obamacare\"). Since you can't call all 327.2 million people ( population ) in the US, you call about 1,000 people ( samples ). Your poll showed that 59% of the registered voters support Obamacare. This does not agree with the actual survey conducted in 2018; 53% favorable, 42% unfavorable ( source ). What could be the source of error? Since (formal) president Obama is a member of the Democratic Party, the voters' response can be affected by their political preference. How could you tell that the 1,000 people you called happened to be mostly Democrats, who's more likely to support Obama's policy, because they share similar political view? The samples you collected could have been biased , but you don't that know for sure. Of course, the voters' response could be affected by many other factors like race, age, place of residence, or financial status. The idea is that, there will always be uncertainty involved with your estimation, because you don't have an access to the entire population. Confidence interval is a technique that quantifies the uncertainty when estimating a population parameter from samples. Notes: Population variance ($\\sigma&#94;2$) vs. Sample variance ($s&#94;2$) Distinction between population parameter and sample parameter is important. In statistics, it is a common practice to denote population variance as $\\sigma&#94;2$, and sample variance as $s&#94;2$. The distinction is important because different equations are used for each. For population: $$ \\text{variance} = \\sigma&#94;2 = \\frac{\\sum(x - \\bar{x})&#94;2}{n} $$ For samples: $$ \\text{variance} = s&#94;2 = \\frac{\\sum(x - \\bar{x})&#94;2}{n-1} $$ The divisor $n-1$ is a correction factor for bias. Note that the correction has a larger proportional effect when $n$ is small than when $n$ is large, which is what we want because the more samples we have, the better the estimation. This idea is well explained on this StackExchange thread . Pythonic Tip: Difference between Numpy variance and Pandas variance Different libraries make different assumption about an input array. The default value of ddof is different for Pandas and Numpy, resulting in different variance. ddof represent degrees of freedom, and setting ddof=True or ddof=1 tells the variance function to calculate sample variance by accounting for the bias factor $n-1$ (recall that in Python, True==1 .) Remember that there is a distinction between Population variance ($\\sigma&#94;2$) vs. Sample variance ($s&#94;2$). If you are confused which library is computing which variance (sample or population), just remember this: whatever library you are using, use ddof=True or ddof=1 to compute sample variance, and use ddof=False or ddof=0 to compute population variance. In [1]: import numpy as np import pandas as pd arr = pd . DataFrame ([ 5 , 3 , 1 , 6 ]) In [7]: # numpy, population arr . values . var () Out[7]: 3.6875 In [8]: # numpy, sample arr . values . var ( ddof = 1 ) Out[8]: 4.916666666666667 In [10]: # pandas, population arr . var ( ddof = 0 ) Out[10]: 0 3.6875 dtype: float64 In [99]: # pandas, sample arr . var () Out[99]: 0 4.916667 dtype: float64 4. Confidence interval of normal distribution Computing confidence interval of a statistic depends on two factors: type of statistic, and type of sample distribution. As explained above , different formulas exist for different type of statistics (Ex: mean, std, variance), and different methods (Ex: boostrapping , credible interval , Box-Cox transformation ) are used for non-normal data set. We will cover confidence interval of mean, difference in mean and variance. 4.1. Confidence interval of mean Confidence interval of mean is used to estimate the population mean from sample data and quantify the related uncertainty. Consider the following figure: Figure 3: Distribution of population and C.I. of mean In figure (3) , assume that the population is normally distributed. Since we don't have an excess to the entire population, we have to guess the population mean (unknown) to the best of our ability using sample data set. We do this by computing the sample mean and constructing its 95% confidence interval . Note that the popular choices of confidence levels are: 90%, 95%, and 99% Assuming normality of population, its sample means are also normally distributed. Let's say that you have a population, and you draw small fractions of it $N$ times. Then, the computed means of $N$ sample sets $\\boldsymbol{\\mu}=(\\mu_1, \\mu_2,..., \\mu_{N-1}, \\mu_N)$ is normally distributed as shown in figure (4) . Their confidence intervals are represented as the black horizontal arrows:. Figure 4: Distribution of sample mean and its C.I. You can see that the confidence interval of $\\mu_5$ does NOT include the green vertical dashed line , 12.31. Let's assume that 12.31 is the true population mean (we never know if this is the actual population mean or not, but let's assume). If we get $\\mu_5$ and its confidence interval as our estimation of the population mean, then our estimation is wrong. There is a 5% chance of this happening, because we set our confidence level as 95%. Note that the width of the confidence intervals (black horizontal arrows) depend on the sample size, as shown in eq (1) The grey area of figure (3) is essentially equivalent to the grey area of figure (4) . $\\mu_1$ = 12.32 is the sample mean, and $\\pm$ 3.56 is the uncertainty related to the sample mean with 95% confidence. The uncertainty is a product of distribution score and standard error of mean. Distribution score essentially tells how many standard error are the limits (8.76 and 15.88) away from the center (12.32). Choosing larger confidence level results in larger confidence interval. This increases the grey area in figure (3) and figure (4) . We convey 95% confidence interval of mean like this: I am 95% confident that the population mean falls between 8.76 and 15.88. If I sample data 20 times, 19 times the sample mean will fall between 8.76 ~ 15.88, but expect that I will be wrong 1 time. Notes: Distribution of various statistics Different statistics exhibit different distributions. Normality of samples does not guarantee normality of its statistics. When the samples are normally distributed, their means are normally distributed, but their variances are chi-square $\\chi&#94;2$ distributed. More discussion about the distribution of variance and $\\chi&#94;2$ distribution is covered below . Note that these assumptions are invalid when samples are non-normal. Source Code For The Figure from scipy import stats import matplotlib.pyplot as plt import numpy as np df_values = [1, 2, 6, 9] linestyles = ['-', '--', ':', '-.'] normal_params = [(10, 1), (11, 1), (10, 2), (10, 3)] x = np.linspace(-1, 20, 1000) fig, ax = plt.subplots(1, 2, figsize=(13.3, 5)) fig.tight_layout() plt.subplots_adjust(left=0.09, right=0.96, bottom=0.12, top=0.93) for df, norm_p, ls in zip(df_values, normal_params, linestyles): ax[1].plot(x, stats.chi2.pdf(x, df, loc=0, scale=1), ls=ls, c='black', label=r'Degrees of freedom$=%i$' % df) ax[0].plot(x, stats.norm.pdf(x, loc=norm_p[0], scale=norm_p[1]), ls=ls, c='black', label='Mean = %d, ' % norm_p[0] + 'Std = %s' % norm_p[1]) ax[0].set_xlim(4, 16) ax[0].set_ylim(-0.025, 0.525) ax[0].set_xlabel('$x$', fontsize=20) ax[0].set_ylabel(r'Probability', fontsize=20) ax[0].set_title(r'Distribution of means: normal distribution', fontsize=20) ax[0].legend(loc='upper left', fontsize=16, framealpha=1, frameon=True) ax[1].set_xlim(0, 10) ax[1].set_ylim(-0.025, 0.525) ax[1].set_xlabel('$\\chi&#94;2$', fontsize=20) ax[1].set_title(r'Distribution of variances: $\\chi&#94;2$ distribution', fontsize=20) ax[1].legend(loc='best', fontsize=16, framealpha=1, frameon=True) If sample data is normal or normal-like distributed, we almost always assume t-distribution to compute confidence interval, as explained below . Then, the confidence interval of mean has the following analytical solution: $$ \\text{C.I.}_{\\text{mean}}: \\quad \\mu \\pm (t_{1-\\frac{\\alpha}{2},df} \\times \\frac{s}{\\sqrt{n}}) \\tag{1}$$ where $\\mu$ : sample mean $\\alpha$ : significance level $n$ : number of samples $df$ : degrees of freedom. In this example, df = $n$ - 1 $s$ : sample standard deviation $t$ : t-score. depends on $\\alpha$ and $df$ Recall that when computing $s$, correction factor ($n-1$) is applied to account for sample bias, as explained above . Pay close attention to the standard error $\\frac{s}{\\sqrt{(n)}}$. As the sample size $n$ increases, the standard error decreases, reducing the range of confidence interval. This is intuitive in a sense that, the more samples we have, the less uncertainty we have with our statistical estimation. The length of the black horizontal arrows in figure (4) depends on the sample size. The larger the sample size, the narrower the width of arrows, and vice versa. Notes: z-score vs t-score You've probably seen mixed use of z-score and t-score for confidence interval during your studies. Long story short, it is safe and almost always better to use t-score than z-score. Z-score ($z_{\\frac{\\alpha}{2}}$) is used for normal distribution, and t-score ($t_{\\frac{\\alpha}{2},df}$) is used for t-distribution. You use z-score if you know the population variance $\\sigma&#94;2$. If not, you use t-score. Since the population variance $\\sigma&#94;2$ is almost never known, you almost always use t-score for confidence interval. After all, the purpose of using confidence interval is to mitigate the issue of Population vs. Samples when estimating population parameter ($\\sigma&#94;2$) from samples. If you know the population parameters, you probably don't need confidence interval in the first place. A natural question is, \"how is it safe to use t-score instead of z-score? Shouldn't I be using z-score since I know that the population is normally distributed, from previous knowledge?\" It is safe to do so because t-distribution converges to normal distribution according to the Centeral Limit Theorem. Recall that t-distribution behaves more and more like a normal distribution as the sample size increases. Google \"95% confidence z-score\" and you will see $z$ = 1.96 at 95% confidence level. On the other hand, t-score approaches 1.96 as its degrees of freedom increases: $\\lim_{df \\to \\infty}t$ = 1.96. For 95% confidence level, $t$ = 2.228 when $n$ - 1 = 10 and $t$ = 2.086 when $n$ - 1 = 20. This is why it is safe to always replace z-score with t-score when computing confidence interval. Pythonic Tip: Computing confidence interval of mean with SciPy We can compute confidence interval of mean directly from using eq (1) . Recall to pass ddof=1 to make sure to compute sample standard deviation $s$, not population standard deviation $\\sigma$, as explained above . We will draw random samples from normal distribution using np.random.normal(). Note that loc is for population mean, and scale is for population standard deviation, and size is for number of samples to draw. In [128]: from scipy import stats import numpy as np np . random . seed ( 42 ) arr = np . random . normal ( loc = 74 , scale = 4.3 , size = 20 ) alpha = 0.05 # significance level = 5% df = len ( arr ) - 1 # degress of freedom = 20 t = stats . t . ppf ( 1 - alpha / 2 , df ) # two-tailed 95% confidence t-score = 2.086 s = np . std ( arr , ddof = 1 ) # sample standard deviation = 2.502 n = len ( arr ) lower = np . mean ( arr ) - ( t * s / np . sqrt ( n )) upper = np . mean ( arr ) + ( t * s / np . sqrt ( n )) In [129]: ( lower , upper ) Out[129]: (71.33139551903422, 75.19543685256606) Or we can compute with scipy.stats.t.interval(). Note that you don't divide alpha by 2, because the function does that for you. Also note that the standard error $\\frac{s}{\\sqrt{n}}$ can be computed with scipy.stats.sem() In [130]: stats . t . interval ( 1 - alpha , len ( arr ) - 1 , loc = np . mean ( arr ), scale = stats . sem ( arr )) Out[130]: (71.33139551903422, 75.19543685256606) Note the default value of loc=0 and scale=1 . This will assume sample mean $\\mu$ to be 0, and standard error $\\frac{s}{\\sqrt{n}}$ to be 1, which assumes standard normal distribution of mean = 0 and standard deviation = 1. This is NOT what we want. In [8]: stats . t . interval ( 1 - alpha , len ( arr ) - 1 ) Out[8]: (-2.093024054408263, 2.093024054408263) 4.2. Confidence interval of difference in mean Confidence interval of difference in mean is not very useful by itself. But it is important to understand how it works, because it forms the basis of one of the most widely used hypothesis test: t-test . Often we are interested in knowing if two distributions are significantly different. In the other words, we want to know if two sample data sets came from the same population by comparing central tendency of populations . A standard approach is to check if the sample means are different. However, this is a misleading approach in a sense that the means of samples are almost always different, even if the difference is microscopic. More useful would be to estimate the difference in a range to account for uncertainty, and compute probability that it is big enough to be of practical importance. T-test checks if the difference is \"close enough\" to zero by computing the confidence interval of difference in means. T-test hypothesis $$ H_0: \\mu_1 - \\mu_2 = 0 \\tag{2}$$ $$ H_1: \\mu_1 - \\mu_2 \\neq 0 \\tag{3}$$ where $\\mu$ : sample mean $H_0$ : null hypothesis — sample means are the same \"enough\" $H_1$ : alternate hypothesis — sample means are \"significantly\" different Note that the above hypothesis tests whether the mean of one group is significantly DIFFERENT from the mean of the other group; we are using two-tailed test. This does not check if the mean of one group is significantly GREATER than the mean of the other group, which uses one-tailed test. Notes: Comparing means of more than two samples with ANOVA Analysis of variance (ANOVA) checks if the means of two or more samples are significantly different from each other. Using t-test is not reliable in cases where there are more than 2 samples. If we conduct multiple t-tests for comparing more than two samples, it will have a compounded effect on the error rate of the result. ANOVA has the following hypothesis: $$ \\begin{align} H_0: &\\mu_1 = \\mu_2 = \\, \\cdots \\, =\\mu_L \\\\[5pt] H_1: &\\mu_a \\neq \\mu_b \\end{align} $$ where $L$ is the number of groups, and $\\mu_a$ and $\\mu_b$ belong to any two sample means of any groups. This article illustrates the concept of ANOVA very well. Figure 5: Distributions of samples In figure (5) , $\\mu$ represents the sample mean. If two sample data sets are from the same population, the distribution of means will be similar \"enough\". If not, they will be \"significantly\" different. It can be visually inspected by the area of overlap. The larger the overlap, the bigger the chance of the two distributions originating from the same population. The more robust way to compare sample means would be to construct the confidence interval of difference in means. If the two samples came from the same population, they should have the similar \"enough\" means. Their difference should be close to zero and satisfy (or fail to reject) the null hypothesis $H_0: \\mu_1 - \\mu_2 = 0$ within a range of uncertainty. Consider the following figure: Figure 6: Distribution of difference in means In figure (6) , the calculated difference in sample means is $\\mu_1 - \\mu_2 = 1.00$ . We deliver the uncertainty related to our estimation of difference in sample means by constructing its 95% confidence interval $[$-1.31 ~ 3.31$]$ . Since the null hypothesis $H_0: \\mu_1 - \\mu_2 = 0$ is within the 95% confidence interval ( grey shaded area ), we accept the null hypothesis; we conclude that the samples have the same means within the uncertainty. However, if the null hypothesis is not within the confidence interval and falls in the 2.5% outliers zone, we reject the null hypothesis and accept the alternate hypothesis $H_1: \\mu_1 - \\mu_2 \\neq 0$ . In the other words, we conclude that the sample means are significantly different. Three variations of confidence interval of difference in means There are three variations of t-test, and therefore there are three variations of confidence interval of difference in means. The difference & application of the three variations are really well-explained in Wikipedia (one of the few that are actually easy to understand, with minimum jargons.) Independent (unpaired) samples, equal variance - Student's t-interval Independent (unpaired) samples, unequal variance - Welch's t-interval Dependent (paired) samples Recall that all t-tests assume normality of data. However, they are pretty robust to non-normality as long as the deviation from normality isn't large. Visualize your distributions to test this. Robustness of t-test to non-normality is discussed in detail below . 4.2.1. Independent (unpaired) samples, equal variance - student's t-interval When you have a reason to believe that samples have nearly equal variances, you can use student's t-test to check if difference in means are significantly different. Note that student's t-test works pretty well even with unequal variances as long as sample sample sizes are equal or nearly equal, and sample sizes are not tiny. However, it is recommended to always use Welch's t-test by assuming unequal variances, as explained below . Use student's t-test if you are ABSOLUTELY sure that the population variances are nearly equal. Confidence interval of difference in mean assuming equal variance (student's t-interval) can be calculated as follows: $$ \\text{C.I.}_{\\Delta \\text{mean}}: \\quad (\\mu_{1}- \\mu_{2}) \\pm (t_{1-\\frac{\\alpha}{2},df} \\times s_p\\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}})\\,, \\quad s_p = \\sqrt{\\frac{(n_1-1)s_{1}&#94;2 + (n_2-1)s_{2}&#94;2}{n_1+n_2-2}} \\tag{4}$$ where $\\mu$ : sample mean $\\alpha$ : significance level $n$ : number of samples $df$ : degrees of freedom $s_p$ : pooled standard deviation $s$ : sample standard deviation $t$ : t-score. depends on $\\alpha$ and degrees of freedom $n-1$ The formula for the pooled standard deviation $s_p$ looks a bit overwhelming, but its just an weighted average standard deviation of two samples, with bias correction factor $n_i-1$ for each sample. Recall that student's t-test assumes equal variances of two samples. You calculate what is assumed to be the common variance (=pooled variance, $s_p&#94;2$) by computing the weighted average from each sample's variance. In eq (4) , $t$-score depends on significance level $\\alpha$ and degrees of freedom $df$. In student's t-test, which assumes equal variance: $$ df = n_1 + n_2 -2 \\tag{5}$$ Pythonic Tip: Computing student's t-interval Unfortunately, SciPy doesn't support computing confidence intereval of difference in mean separately. It is incorporated into computing t-statistic and p-value of t-test, but users can't access its underlying confidence interval. Note that in R, users have access to the CI of difference in means. We can compute CI of difference in means assuming equal variance with eq (4) . Don't forget to compute sample variance, instead of population variance by setting ddof=1 as explained above . In [182]: from scipy import stats import numpy as np In [183]: x1 = [ 12.9 , 10.2 , 7.4 , 7.0 , 10.5 , 11.9 , 7.1 , 9.9 , 14.4 , 11.3 ] x2 = [ 10.2 , 6.9 , 10.9 , 11.0 , 10.1 , 5.3 , 7.5 , 10.3 , 9.2 , 8.8 ] alpha = 0.05 # significance level = 5% n1 , n2 = len ( x1 ), len ( x2 ) # sample sizes s1 , s2 = np . var ( x1 , ddof = 1 ), np . var ( x2 , ddof = 1 ) # sample variances s = np . sqrt ((( n1 - 1 ) * s1 + ( n2 - 1 ) * s2 ) / ( n1 + n2 - 2 )) # pooled standard deviation df = n1 + n2 - 2 # degrees of freedom t = stats . t . ppf ( 1 - alpha / 2 , df ) # two-tailed 95% confidence t-score lower = ( np . mean ( x1 ) - np . mean ( x2 )) - t * np . sqrt ( 1 / len ( x1 ) + 1 / len ( x2 )) * s upper = ( np . mean ( x1 ) - np . mean ( x2 )) + t * np . sqrt ( 1 / len ( x1 ) + 1 / len ( x2 )) * s In [184]: ( lower , upper ) Out[184]: (-0.8520326742900641, 3.332032674290068) The 95% confidence interval of difference in means has 0 within its interval. This means that the null hypothesis, $H_0: \\mu_1 - \\mu_2 = 0$ in figure (6) , falls within the interval and we fail to reject the null hypothesis. We conclude that the sample means are not significantly different. We can confirm this by running a formal hypothesis testing with scipy.stats.ttest_ind() , and setting equal_var=True . Note that this assumes independent t-test with pooled variance, which is equivalent to student's t-test. In [185]: stats . ttest_ind ( x1 , x2 , equal_var = True ) Out[185]: Ttest_indResult(statistic=1.2452689491491107, pvalue=0.22900078577218805) The computed pvalue=0.229 is bigger than the significance level of alpha = 0.05 , and therefore we fail to reject the null hypothesis, which is consistent with the conclusion drawn from the confidence interval of difference in mean. Checking results with R : a <- c(12.9, 10.2, 7.4, 7.0, 10.5, 11.9, 7.1, 9.9, 14.4, 11.3) b <- c(10.2, 6.9, 10.9, 11.0, 10.1, 5.3, 7.5, 10.3, 9.2, 8.8) t.test(a, b, var.equal = TRUE) # Two Sample t-test # data: a and b # t = 1.2453, df = 18, p-value = 0.229 # 95 percent confidence interval: # -0.8520327 3.3320327 # sample estimates: # mean of x mean of y # 10.26 9.02 4.2.2. Independent (unpaired) samples, unequal variance - Welch's t-interval When comparing central tendency of normal distributions, it is safer, and therefore recommended to always use Welch's t-test, which assumes unequal variances of samples, as explained below . Equal variance t-test is not robust when population variances are different, but unequal variances are robust even when population variances are equal. Confidence interval of difference in mean assuming unequal variance (Welch's t-interval) can be calculated as follows: $$ \\text{C.I.}_{\\Delta \\text{mean}}: \\quad (\\mu_{1}- \\mu_{2}) \\pm (t_{1-\\frac{\\alpha}{2},df} \\times \\sqrt{\\frac{s_1&#94;2}{n_1}+\\frac{s_2&#94;2}{n_2}}) \\tag{6}$$ where $\\mu$ : sample mean $\\alpha$ : significance level $n$ : number of samples $df$ : degrees of freedom $s$ : sample standard deviation $t$ : t-score. depends on $\\alpha$ and degrees of freedom $n-1$ The formula is very similar to student's t-interval. There are two main differences: 1. We use each sample's own variance $s_1&#94;2$ and $s_2&#94;2$, instead of pooled (weighted average) variance $s_p&#94;2$. 2. Degrees of freedom $df$ is computed with eq (7). $$ df = \\frac{(\\frac{s&#94;2_1}{n_1} + \\frac{s&#94;2_2}{n_2})&#94;2}{\\frac{(s&#94;2_1/n_1)&#94;2}{n_1-1} + \\frac{(s&#94;2_2/n_2)&#94;2}{n_2-1}} \\tag{7}$$ Pythonic Tip: Computing Welch's t-interval The procedure is very similar to Computing student's t-interval . We will compute confidence interval of difference in mean assuming unequal variance, with eq (6). Although Scipy supports computing t-statistic for Welch's t-test, it doesn't support a function that allows us to compute Welch's t-interval. We will have to write our own codes to compute it. Don't forget to compute sample variance, instead of population variance by setting ddof=1 as explained above . In [186]: from scipy import stats import numpy as np In [187]: x1 = [ 12.9 , 10.2 , 7.4 , 7.0 , 10.5 , 11.9 , 7.1 , 9.9 , 14.4 , 11.3 ] x2 = [ 10.2 , 6.9 , 10.9 , 11.0 , 10.1 , 5.3 , 7.5 , 10.3 , 9.2 , 8.8 ] alpha = 0.05 # significance level = 5% n1 , n2 = len ( x1 ), len ( x2 ) # sample sizes s1 , s2 = np . var ( x1 , ddof = 1 ), np . var ( x2 , ddof = 1 ) # sample variances df = ( s1 / n1 + s2 / n2 ) ** 2 / (( s1 / n1 ) ** 2 / ( n1 - 1 ) + ( s2 / n2 ) ** 2 / ( n2 - 1 )) # degrees of freedom t = stats . t . ppf ( 1 - alpha / 2 , df ) # two-tailed 95% confidence t-score lower = ( np . mean ( x1 ) - np . mean ( x2 )) - t * np . sqrt ( 1 / len ( x1 ) + 1 / len ( x2 )) * s upper = ( np . mean ( x1 ) - np . mean ( x2 )) + t * np . sqrt ( 1 / len ( x1 ) + 1 / len ( x2 )) * s In [188]: ( lower , upper ) Out[188]: (-0.8633815129922358, 3.3433815129922397) The 95% confidence interval of difference in means has 0 within its interval. This means that the null hypothesis, $H_0: \\mu_1 - \\mu_2 = 0$ in figure (6) , falls within the interval and we fail to reject the null hypothesis. We conclude that the sample means are not significantly different. We can confirm this by running a formal hypothesis testing with scipy.stats.ttest_ind() , and setting equal_var=False . Note that this assumes independent t-test with pooled variance, which is equivalent to student's t-test. In [189]: stats . ttest_ind ( x1 , x2 , equal_var = False ) Out[189]: Ttest_indResult(statistic=1.245268949149111, pvalue=0.23018336828903668) The computed pvalue=0.230 is bigger than the significance level of alpha = 0.05 , and therefore we fail to reject the null hypothesis, which is consistent with the conclusion drawn from the confidence interval of difference in mean. Checking results with R : a <- c(12.9, 10.2, 7.4, 7.0, 10.5, 11.9, 7.1, 9.9, 14.4, 11.3) b <- c(10.2, 6.9, 10.9, 11.0, 10.1, 5.3, 7.5, 10.3, 9.2, 8.8) t.test(a, b, var.equal = FALSE) # Welch Two Sample t-test # data: a and b # t = 1.2453, df = 16.74, p-value = 0.2302 # alternative hypothesis: true difference in means is not equal to 0 # 95 percent confidence interval: # -0.8633815 3.3433815 # sample estimates: # mean of x mean of y # 10.26 9.02 4.2.3. Dependent (paired) samples - Paired t-interval This test is used when the samples are dependent; that is, when there is only one sample that has been tested twice (repeated measures) or when there are two samples that have been matched or \"paired\" (paired or unpaired? read below. ) Confidence interval of difference in means assuming paired samples can be calculated as follows: $$ \\text{C.I.}_{\\Delta \\text{mean}}: \\quad \\bar{d} \\pm (t_{1-\\frac{\\alpha}{2}, df} \\times \\frac{s_d}{\\sqrt{n}})\\tag{8}$$ where $\\bar{d}$ : average of sample differences $\\alpha$ : significance level $n$ : number of samples $df$ : degrees of freedom $s$ : standard deviation of sample differences $t$ : t-score. depends on $\\alpha$ and degrees of freedom $n-1$ The equation is very similar to eq (1) , except that we are computing mean and standard deviation of differences between before & after state of test subjects. Let's try to understand this with an example. A school develops a tutoring program to improve the SAT scores of high school students. A school requires students to take tests before & after tutoring, and checks if the tutoring had a significant impact on the SAT scores of students. Because the test subjects are compared to themselves, not anyone elses, the measurements taken before & after the training are not independent. To compute dependent t-interval, we compute differences of test scores before & after tutoring: Student # $X_1$ $X_2$ $X_1$ - $X_2$ 1 1480 1510 -30 2 1280 1460 -180 3 890 1320 -430 4 340 700 -360 5 1550 1550 0 6 1230 1420 -190 7 1010 1340 -330 8 1590 1570 20 9 1390 1500 -110 10 980 1300 -320 We find $\\bar{d}$ = -193.0, and $s_d$ = 161.7. These values are plugged into eq (8). Degrees of freedom $df$ for dependent t-interval can be computed with: $$ df = n - 1 \\tag{9}$$ Unlike independent t-test, in which two samples can have different sample sizes $n_1$ and $n_2$, depedent t-test has only one sample size, because the test subjects are compared to themselves. Also note that dependent t-test assumes difference of test scores to be normally distributed, not test scores of students themselves. But as long as the test scores are normally distributed, the difference of test scores will also be normally distributed due to the property of normal distributions. Pythonic Tip: Computing paired t-interval Although Scipy supports computing t-statistic for dependent t-test, it doesn't support a function that allows us to compute dependent t-interval. We will have to write our own codes to compute it. Don't forget to compute sample standard devaition, instead of population standard deviation by setting ddof=1 as explained above . In [190]: from scipy import stats import numpy as np In [191]: x1 = np . array ([ 1480 , 1280 , 890 , 340 , 1550 , 1230 , 1010 , 1590 , 1390 , 980 ]) x2 = np . array ([ 1510 , 1460 , 1320 , 700 , 1550 , 1420 , 1340 , 1570 , 1500 , 1300 ]) alpha = 0.05 # significance level = 5% d_bar = np . mean ( x1 - x2 ) # average of sample differences s_d = np . std ( x1 - x2 , ddof = 1 ) # sample standard deviation of sample differences n = len ( x1 ) # sample size df = n - 1 # degrees of freedom t = stats . t . ppf ( 1 - alpha / 2 , df ) # two-tailed 95% confidence t-score lower = d_bar - t * s_d / np . sqrt ( n ) upper = d_bar + t * s_d / np . sqrt ( n ) In [192]: ( lower , upper ) Out[192]: (-308.64567899681356, -77.35432100318641) The 95% confidence interval of difference in means for dependent samples does not have 0 within its interval. This means that the null hypothesis, $H_0: \\mu_1 - \\mu_2 = 0$ in figure (6) , does not fall within the interval. Instead, our estimation falls within the 2.5% outlier zone on the left, $H_1: \\mu_1 - \\mu_2 \\neq 0$ . We reject the null hypothesis $H_0$, and accept the alternate hypothesis $H_1$. We conclude that the sample means are significantly different. We can confirm this by running a formal hypothesis testing with scipy.stats.ttest_rel(). Note that this assumes dependent t-test. In [193]: stats . ttest_rel ( x1 , x2 ) Out[193]: Ttest_relResult(statistic=-3.7752930865755987, pvalue=0.004380623368522125) The computed pvalue=0.004 is smaller than the significance level of alpha = 0.05 , and therefore we reject the null hypothesis and accept the alternate hypothesis, which is consistent with the conclusion drawn from the confidence interval of difference in mean. Notes : The above hypothesis testing answers the question of \"Did this tutoring program had a significant impact on the SAT scores of students?\". However, in cases like this, a more intuitive question is \"Did this tutoring program significantly improve the SAT scores of students?\" The former uses two-tailed test, and the latter uses one-tailed test, and the procedures for them are a different. They are a little confusing. More explanation of one-tailed test vs two-tailed test is covered below . Checking results with R : x1 = c(1480, 1280, 890, 340, 1550, 1230, 1010, 1590, 1390, 980) x2 = c(1510, 1460, 1320, 700, 1550, 1420, 1340, 1570, 1500, 1300) t.test(x1, x2, paired=TRUE) # Paired t-test # data: x1 and x2 # t = -3.7753, df = 9, p-value = 0.004381 # alternative hypothesis: true difference in means is not equal to 0 # 95 percent confidence interval: # -308.64568 -77.35432 # sample estimates: # mean of the differences # -193 Notes: Deciding which t-test to use Equal or unequal variance? Long story short, always assume unequal variance of samples when using t-test or constructing confidence interval of difference in means. Student's t-test is used for samples of equal variance, and Welch's t-test is used for samples of unequal variance. A natural question is, how do you know which test to use? While there exist techniques to check homogeneity of variances (f-test, Barlett's test, Levene's test), it is dangerous to run hypothesis testing for equality of variances to decide which t-test to use (student's t-test or Welch's t-test), because it increases Type I error (asserting something that is absent, false positive). This is shown by Moser and Stevens (1992) and Hayes and Cai (2010). Kubinger, Rasch and Moder (2009) argue that when the assumptions of normality and homogeneity of variances are met, Welch's t-test performs equally well, but outperforms when the assumptions are not met. Ruxton (2006) argues that the \"unequal variance t-test should always be used in preference to the Student's t-test\" (Note: what he means by \"always\" is assuming normality of distribution) Also note that R uses Welch's t-test as the default for the t.test() function. Independent (unpaired) or dependent (paired) samples? Paired t-test compares the same subjects at 2 different times . Unpaired t-test compares two different subjects. Samples are independent (unpaired) if one measurement is taken on different groups. For example in medical treament, group A is a control group, and is given a placebo with no medical effect. Group B is a test group, and receives a prescribed treatment with expected medical effect. Health check is applied on two groups, and the measurements are recorded. We say that the measurement from group A is independent from that of group B. Samples are dependent (paired) when repeated measures are taken on the same or related subjects. For example, there may be instances of the same patients being tested repeatedly - before and after receiving a particular treatment. In such cases, each patient is being used as a control sample against themselves. This method also applies to cases where the samples are related in some manner or have matching characteristics, like a comparative analysis involving children, parents or siblings. If you have a reason to believe that samples are correlated in any ways, it is recommended to use dependent test to reduce the effect of confounding factors . 4.3. Confidence interval of variance Confidence interval of variance is used to estimate the population variance from sample data and quantify the related uncertainty. C.I. of variance is seldom used by itself, but rather used in conjunction with f-test , which tests equality of variances of different populations. Similar to how the confidence interval of difference in mean forms the foundation of t-test , C.I. of variance forms the foundation of f-test. In the field of statistics and machine learning, the equality of variance is an important assumption when choosing which technique to use. For example, when comparing the means of two samples, student's t-test should not be used when you have a reason to believe that the two samples have different variances. Personally, I found f-test to be useful for the purpose of reading and understanding scientific papers, as many of the papers I have read use f-test to test their hypothesis, or use a variation of f-test for more advanced techniques. I mentioned that different statistics exhibit different distributions above . When a sample data set originates from a normal distribution, its sample means are normally distributed as shown in figure 4 . On the other hand, its sample variances are chi-square ($\\chi&#94;2$) distributed as shown in figure ??? . Figure ???: 95% confidence interval of variance. Source Code For Figure (?) from scipy import stats import matplotlib.pyplot as plt import numpy as np df = 9 x = np.linspace(-1, 28, 1000) y = stats.chi2.pdf(x, df, loc=0, scale=1) right_tail = stats.chi2.ppf(1 - 0.025, df) left_tail = stats.chi2.ppf(1 - 0.975, df) plt.style.use('seaborn-whitegrid') fig, ax = plt.subplots(figsize=(12, 5)) ax.plot(x, y, c='black', label='Degrees of freedom = %d' % df) ax.set_xlabel('$\\chi&#94;2$', fontsize=17) ax.set_ylabel(r'Probability', fontsize=17) ax.set_title(r'$\\chi&#94;2\\ \\mathrm{Distribution}$, df = %d' % df, fontsize=17) ax.fill_between(x, 0, y, where=(np.array(x) > min(x)) & (np.array(x) <= left_tail), facecolor='grey') ax.fill_between(x, 0, y, where=(np.array(x) > left_tail) & (np.array(x) < right_tail), facecolor='lightgrey') ax.fill_between(x, 0, y, where=(np.array(x) > right_tail) & (np.array(x) <= max(x)), facecolor='grey') ax.grid(False) ax.text(22, 0.008, '2.5% outlier', fontsize=13) ax.text(-2, 0.008, '2.5% outlier', fontsize=13) ax.text(0.5, 0.04, '$\\chi&#94;2_{.975} = %.2f$' % left_tail, fontsize=14, bbox=dict(boxstyle='round', facecolor='white')) ax.text(16.5, 0.015, '$\\chi&#94;2_{.025} = %.2f$' % right_tail, fontsize=14, bbox=dict(boxstyle='round', facecolor='white')) ax.text(20, 0.08, '$\\chi&#94;2_{.975} \\leq \\chi&#94;2 \\leq \\chi&#94;2_{.025}$', fontsize=16) ax.text(20, 0.06, '$2.70 \\leq \\chi&#94;2 \\leq 19.02$', fontsize=16) ax.text(6, 0.05, '95% confidence interval', fontsize=16) ax.text(6, 0.04, 'of variance', fontsize=16); Notes: Chi-square $\\chi&#94;2$ distribution Chi-square $\\chi&#94;2$ distribution is a function of degrees of freedom $df$ . It is a special case of the gamma distribution and is one of the most widely used probability distributions in inferential statistics, notably in hypothesis testing or in construction of confidence intervals. It is used in the common chi-square goodness of fit test of an observed data set to a theoretical one. Let's say that there's a company that prints baseball cards. The company claims that 30% of the cards are rookies, 60% veterans but not All-Stars, and 10% are veteran All-Stars. Suppose that you purchased a deck of 100 cards. You found out that the card deck has 50 rookies, 45 veterans, and 5 All-Stars. Is this consistent with the company's claim? An answer to this question is explained in detail here using the chi-squared goodness of fit test. When samples have a normal distribution, some of their statistics can be described by $\\chi&#94;2$ distributions. For example, the Mahalanobis distance follows $\\chi&#94;2$ distribution when samples are normally distributed, and can be used for multivariate outlier detection using $\\chi&#94;2$ hypothesis test. Variance of samples also follows $\\chi&#94;2$ distributions when samples are normally distributed, and can be used to construct the confidence interval of variances with eq (???) . By the central limit theorem, a $\\chi&#94;2$ distribution converges to a normal distribution for large sample size $n$ . For many practical purposes, for $n$ > 50 the distribution is sufficiently close to a normal distribution for the difference to be ignored. Note that the sampling distribution of $ln(\\chi&#94;2)$ converges to normality much faster than the sampling distribution of $\\chi&#94;2$ as the logarithm removes much of the asymmetry. Source Code For The Figure from scipy import stats import matplotlib.pyplot as plt import numpy as np df_values = [1, 2, 6, 9] linestyles = ['-', '--', ':', '-.'] x = np.linspace(-1, 20, 1000) fig, ax = plt.subplots(figsize=(6.6666666, 5)) fig.tight_layout() plt.subplots_adjust(left=0.09, right=0.96, bottom=0.12, top=0.93) for df, ls in zip(df_values, linestyles): ax.plot(x, stats.chi2.pdf(x, df, loc=0, scale=1), ls=ls, c='black', label=r'Degrees of freedom$=%i$' % df) ax.set_xlim(0, 10) ax.set_ylim(0, 0.5) ax.set_xlabel('$\\chi&#94;2$', fontsize=14) ax.set_ylabel(r'Probability', fontsize=14) ax.set_title(r'$\\chi&#94;2\\ \\mathrm{Distribution}$') ax.legend(loc='best', fontsize=11, framealpha=1, frameon=True) In [116]: right_tail = stats . chi2 . ppf ( 1 - 0.025 , df ) left_tail = stats . chi2 . ppf ( 1 - 0.975 , df ) In [117]: right_tail Out[117]: 19.02276779864163 In [115]: left_tail_975 = stats . chi2 . ppf ( 1 - 0.975 , df ) Variance follows chi-squared distribution, if samples are normally distributed. P-value propagates toward left Equation, depends on alpha, degree of freedom Means can be the same, but their std can be different. One team is pretty consistent from game to game, but the other team fluctuates a lot. Which team has the smaller score variance? -> plays a very important role in quality assurance and operations management Variance is chi-square distributed. But how is it related to the actual data we collect? $$\\chi&#94;2 = \\frac{(n-1)s&#94;2}{\\sigma&#94;2}$$$$\\chi_{.975}&#94;{2} < \\chi&#94;2 < \\chi_{.025}&#94;{2}$$$$ \\text{C.I.}_{\\text{variance}}: \\frac{(n-1)s&#94;{2}}{\\chi&#94;{2}_{\\frac{\\alpha}{2}}} \\leq \\sigma&#94;2 \\leq \\frac{(n-1)s&#94;{2}}{\\chi&#94;{2}_{1-\\frac{\\alpha}{2}}}$$ stats.chi2.ppf(0.975, 11) = 21.92 We are estimating population variance. This volatility is one way of interpreting risk. The trade-off of risk vs. reward. Used in conjunction with more advanced tasks. In [2]: import matplotlib.pyplot as plt from scipy import stats import numpy as np conf_pinkman = stats . t . interval ( 1 - 0.05 , 18 - 1 , loc = 96.2 , scale = 4 / np . sqrt ( 18 )) conf_white = stats . t . interval ( 1 - 0.05 , 21 - 1 , loc = 99.1 , scale = 3 / np . sqrt ( 21 )) plt . style . use ( 'seaborn-whitegrid' ) fig , ax = plt . subplots ( figsize = ( 5 , 2 )) ax . errorbar ( 99.1 , 1 , xerr = ( conf_white [ 1 ] - conf_white [ 0 ]) / 2 , fmt = 'o' , markersize = 8 , capsize = 5 , label = 'Mr. White \\' s' , color = 'grey' ) ax . errorbar ( 96.2 , 0 , xerr = ( conf_pinkman [ 1 ] - conf_pinkman [ 0 ]) / 2 , fmt = 'o' , markersize = 8 , capsize = 5 , label = 'Mr. Pinkman \\' s' , color = 'k' ) ax . set_ylim ( - 0.6 , 1.6 ) ax . fill_betweenx ([ 1 , 0 ], conf_white [ 0 ], conf_pinkman [ 1 ], facecolor = 'lightgrey' , alpha = 0.3 ) ax . legend ( loc = 'best' , fontsize = 11 , framealpha = 1 , frameon = True ) ax . set_xlabel ( 'Purity (%)' , fontsize = 12 ) ax . yaxis . set_major_formatter ( plt . NullFormatter ()) fig . tight_layout (); Robustness of variance to non-normality Standard deviations are a bit more sensitive to outliers than means are -- when you put in an outlier, you tend to push the one-sample t-statistic toward 1 or -1. https://stats.stackexchange.com/questions/193117/why-is-high-positive-kurtosis-problematic-for-hypothesis-tests Difference of the confidence interval constructed with t-test vs bootstrap for skewed data In [ ]: Robustness of confidence intervals to non-normality All of the discussions above assume normality of data set. Conclusions drawn from confidence intervals and hypothesis testing may be inaccurate, or completely wrong if the underlying assumptions are not met. This sections discusses why the assumption of normality is important, robustness of statistical estimations in regards to deviation from normality, and non-parametric alternatives. Measuring central tendency of distributions We discussed how to compute confidence interval of mean and confidence interval of difference in means. But have you thought about why statisticians bother specifically about the means? Often times the ultimate goal is not to compute a mean of a distribution, but to compute a measure of central tendency of a distribution. A measure of central tendency is a single value that attempts to describe a set of data by identifying the central position within that set of data. The mean is the measure of central tendency that you are the most familiar with, but there are others, such as the median and the mode. Point estimation Mean is not a good measure of central tendency when there is a sign of deviation from normality, which can be characterized by skewness (asymmetry) and kurtosis (heavy-tails). Consider the following figures: Figure ??: Central tendency of distributions In figure (??) (a) , there is no skewness, making the distribution symmetric. In this case, mean is a good measure of central tendency, along with median and mode — they are all equivalent. You can compute its confidence interval of mean using eq (1) , and use it as a measure of central tendency, thanks to the symmetry of the distribution. However, when distributions have non-zero skewness, as in figure (??) (b) , the assumption of normality is violated. Depending on how bad the asymmetry is, you can still use eq (1) , because confidence interval of mean is robust to mild asymmetry. However, if a distribution has non-negligible skewness, interval computed with eq (1) introduces bias to one side of a distribution. Although there exists non-parametric alternatives like Bootstrap and credible interval, you may want to take a step back and reconsider the purpose of your estimation. Are you really interested in the mean , or the central tendency of your distribution? In a case like figure (??) (b) , perhaps you are more interested in the median, as it's a better measure of central tendency of asymmetric distributions. Visualize your distribution to see what your true interest is. Comparison of distributions Similar idea applies when you want to compare two distributions. Let's say that you want to decide if two samples came from the same population by comparing their confidence interval of means. Consider the following figure of two non-normal sample distributions: Figure ??: Confidencen interval of means for non-normal distributions Note that the 95% confidence interval of means in figure (??) are asymmetric about their respective sample means, because they are computed with non-parametric alternatives. Since there is an overlap of the two intervals, you can conclude that the sample means are not significantly different. Furthermore, even the variances of the samples are the same; I generated the plots so that they have the same variances. Now, you know that the samples have equal means and variances within the range of uncertainty. Can you conclude that the two samples came from the same population? Clearly not, because their central locations are far apart from each other. Their central tendencies are better described by their medians than their means. In this case, if you are using confidence confidence interval of means and variances to check if two samples came from the same population, your approach is wrong. This kind of approach assumes normality of data. Any approach that makes a certain assumption of data fails when that assumption is violated. If your goal is to find out if the two samples originated from the same population, you may want to use non-parametric alternatives, such as Mann-Whitney test or Kruskall-Wallis test; they are geared towards comparing central tendency of distributions, not means or variances. Robustness of t-test T-test loses some of its statistical power when there is a sign of deviation from normality, which can be characterized by skewness (asymmetry) and kurtosis (heavy-tails). Why? It's because t-test is a test of means, and means are not good measures of central tendency of asymmetric distributions, as I explained here . Let's say that you want to decide if two samples came from different populations by comparing the confidence interval of means. Consider the following figure: -------- image -------- T-test is fairly resistant to moderate deviations from normality — visualize your distribution to test this. To be specific, it is sensitive to asymmetry of the distribution, but not much to kurtosis. As a rule of thumb (not a law of nature), inference about means is sensitive to skewness and inference about variances is sensitive to kurtosis asymptotic expansion https://stats.stackexchange.com/questions/2492/is-normality-testing-essentially-useless/30053#30053 T-test is fairly resistent to moderate deviations https://www.johndcook.com/blog/2018/05/11/two-sample-t-test/ That said, the t-test can tolerate moderate skewness and heavy-tailedness (though in the latter case your actual significance levels will tend to be lower than the nominal 𝛼). Non-parametric alternatives are 95% powerful of t-test, when data are normal. Robustness of confidence interval of variance Non-parametric alternatives In [6]: from scipy import stats % matplotlib inline stats . chi2 . ppf ( 0.025 , 11 ) Out[6]: 3.8157482522361 In [ ]: In [ ]: In [177]: x1 = [ 12.9 , 10.2 , 7.4 , 7.0 , 10.5 , 11.9 , 7.1 , 9.9 , 14.4 , 11.3 ] x2 = [ 10.2 , 6.9 , 10.9 , 11.0 , 10.1 , 5.3 , 7.5 , 10.3 , 9.2 , 8.8 ] alpha = 0.05 # significance level = 5% n1 , n2 = len ( x1 ), len ( x2 ) # sample sizes s1 , s2 = np . var ( x1 , ddof = 1 ), np . var ( x2 , ddof = 1 ) # sample variances df = ( s1 / n1 + s2 / n2 ) ** 2 / (( s1 / n1 ) ** 2 / ( n1 - 1 ) + ( s2 / n2 ) ** 2 / ( n2 - 1 )) # degrees of freedom t = stats . t . ppf ( 1 - alpha , df ) lower = ( np . mean ( x1 ) - np . mean ( x2 )) - t * np . sqrt ( 1 / len ( x1 ) + 1 / len ( x2 )) * s lower Out[177]: -0.49379818720606283 In [3]: from scipy.stats import skewnorm from matplotlib import pyplot as plt import numpy as np import scipy.stats a = 100 data = skewnorm . rvs ( a , size = 1000 ) In [91]: print ( np . mean ( data )) print ( np . median ( data )) fig , ax = plt . subplots () _ = ax . hist ( data , bins = 50 , edgecolor = 'black' ); ax . axvline ( 1 ) 0.8381086010164204 0.7177974832846665 Out[91]: In [321]: % matplotlib inline data = skewnorm . rvs ( 20 , size = 1000 ) density = stats . gaussian_kde ( noise ) mean = np . mean ( data ) median = np . median ( data ) fig , ax = plt . subplots () n , x , _ = ax . hist ( data , bins = 50 , density = True , edgecolor = 'black' ) ax . plot ( x , density ( x )) ax . axvline ( mean , color = 'red' ) ax . axvline ( median , color = 'green' ) Out[321]: In [322]: lower , upper = stats . t . interval ( 1 - 0.05 , len ( data ) - 1 , loc = np . mean ( data ), scale = stats . sem ( data )) mean = np . mean ( data ) print (( lower , upper )) print ( mean ) (0.7714451906907809, 0.8480109036057044) 0.8097280471482426 In [323]: incre = 5 temp , lmbda = stats . boxcox ( data + incre ) lower_b , upper_b = stats . t . interval ( 1 - 0.05 , len ( temp ) - 1 , loc = np . mean ( temp ), scale = stats . sem ( temp )) lower_b , upper_b = inv_boxcox ( lower_b , lmbda ), inv_boxcox ( upper_b , lmbda ) mean_b = inv_boxcox ( mean , lmbda ) In [324]: print (( lower_b - incre , upper_b - incre )) (0.6609200748348814, 0.7272585524637014) In [330]: plt . hist ( temp , density = True , bins = 'auto' ); fig , ax = plt . subplots () prob = stats . probplot ( temp , dist = stats . norm , plot = ax ) In [290]: from scipy.special import inv_boxcox In [292]: from sklearn.utils import resample boots = np . array ([ resample ( data , replace = True ) for _ in range ( 100 )]) a = sorted ( np . mean ( boots , axis = 1 )) print ( a [ 5 ], a [ 95 ]) print ( a [ 50 ]) 0.7418236409304393 0.796416903031344 0.7675645387929715 In [331]: # sample data generation np . random . seed ( 42 ) data = sorted ( stats . lognorm . rvs ( s = 0.5 , loc = 1 , scale = 1000 , size = 1000 )) # fit lognormal distribution shape , loc , scale = stats . lognorm . fit ( data , loc = 0 ) pdf_lognorm = stats . lognorm . pdf ( data , shape , loc , scale ) In [333]: plt . plot ( data , pdf_lognorm ) plt . hist ( data , density = True , bins = 'auto' ); plt . hist ( data , density = True , bins = 'auto' ); fig , ax = plt . subplots () prob = stats . probplot ( data , dist = stats . norm , plot = ax ) In [ ]: In [334]: from sklearn.utils import resample boots = np . array ([ resample ( data , replace = True ) for _ in range ( 100 )]) a = sorted ( np . mean ( boots , axis = 1 )) print ( a [ 5 ], a [ 95 ]) print ( a [ 50 ]) 1117.553360410937 1168.6710419552019 1142.8236697715936 In [335]: lower , upper = stats . t . interval ( 1 - 0.05 , len ( data ) - 1 , loc = np . mean ( data ), scale = stats . sem ( data )) mean = np . mean ( data ) print (( lower , upper )) print ( mean ) (1103.6610566132003, 1180.2601663391847) 1141.9606114761925 In [ ]: Measure of central tendency The median is usually preferred to other measures of central tendency when your data set is skewed (i.e., forms a skewed distribution) or you are dealing with ordinal data. It is usually inappropriate to use the mean in such situations where your data is skewed. You would normally choose the median or mode, with the median usually preferred. In [ ]: In [ ]: Robustness of confidence interval to non-normality All of the discussions above assume normality of data set. Conclusions drawn from confidence intervals and hypothesis testing may be inaccurate, or completely wrong if the underlying assumptions are not met. This sections discusses why the assumption of normality is important, robustness of statistical estimations in regards to deviation from normality, and non-parametric alternatives. Measuring central tendency of distributions We discussed how to compute confidence interval of mean and confidence interval of difference in means. But have you thought about why statisticians bother specifically about the means? The ultimate goal is not to compute a mean of distributions, but to compute a measure of central tendency of distributions. A measure of central tendency is a single value that attempts to describe a set of data by identifying the central position within that set of data. As such, measures of central tendency are sometimes called measures of central location. The mean is the measure of central tendency that you are the most familiar with, but there are others, such as the median and the mode. Robustness of confidence interval of mean Robustness of confidence interval of variance Non-parametric alternatives In [ ]: In [ ]: 1. Is it for point estimation? Is it for hypothesis testing? The t tends to have reasonably good power relative to the Mann-Whitney for light-tailed distributions ... and can have really bad power for heavy-tailed ones. Skewness tends to be compounded with heavy tails T-test is robust to mild skewness, that is, mild non-normality. have to note that all of the knowledge I just imparted is somewhat obsolete; now that we have computers, we can do better than t-tests. As Frank notes, you probably want to use Wilcoxon tests anywhere you were taught to run a t-test. Notes: Robustness of confidence interval to non-normality t-test is fairly resistent to moderate deviations from normality If distribution is unacceptably not normal, use confidence interval of median, but only if your goal is to compare central tendency. Overall, the two sample t-test is reasonably power-robust to symmetric non-normality When the two samples are mildly skew in the same direction, the one-tailed t-test is no longer unbiased. Confidence interval of mean can stand some violation of normality. But confidecne interval of variance is very susceptible. Sample Sizes Illustrate confidence intervals of every stats, with previously given s, mu. This comparison helps to determine how likely the difference between the means occurred by chance or whether the data sets really have intrinsic differences. The t-test questions whether the difference between the groups represents a true difference in the study or if it is likely a meaningless statistical difference. Python confidence interval of difference in means https://stackoverflow.com/questions/31768464/confidence-interval-for-t-test-difference-between-means-in-python/34516534#34516534 Power of non-parametric methods on normal data Bootstrapping CLT does not apply to how many resampling you do. It applied only on the original dataset. In [ ]: In [29]: np . var ( y ) Out[29]: 8.181868181125685e-05 In [30]: np . var ( y2 ) Out[30]: 8.181868181125686e-05 In [22]: x = np . arange ( 0 , 100 , 0.1 ) y = stats . beta . pdf ( x , a = 2 , b = 5 , loc = 0 , scale = 100 ) x2 = np . arange ( 0 , 100 , 0.1 ) y2 = stats . beta . pdf ( x , a = 5 , b = 2 , loc = 0 , scale = 100 ) fig , ax = plt . subplots ( figsize = ( 8 , 6 )) ax . plot ( x , y , x2 , y2 , linewidth = 4 ) Out[22]: [ , ] In [ ]: In [ ]: In [ ]: One-tail vs Two-tail Customer: wants at least 355 ml in water $$\\text{Quantity of water} \\geq 355 ml$$ Manufacturer: exactly 355 ml $$\\text{Quantity of water} = 355 ml$$ Collect 50 bottles, and test our assumption 1 sample vs 2 samples Am I testing assumption, or the status quo that already exists? -> 1 sample? Am I testing new claim or assertion beyoned what I already know? -> 2 samples? Frequently Asked Questions What is significance level? What is confidence level? What is standard error? What is distribution score? What should I do if the sample data is not normally distributed? CI of lognormal distribution https://amstat.tandfonline.com/doi/full/10.1080/10691898.2005.11910638#.XQvVFtNKhQI In [25]: x1 = [ 12.9 , 10.2 , 7.4 , 7.0 , 10.5 , 11.9 , 7.1 , 9.9 , 14.4 , 11.3 ] x2 = [ 10.2 , 6.9 , 10.9 , 11.0 , 10.1 , 5.3 , 7.5 , 10.3 , 9.2 , 8.8 ] In [26]: stats . t . interval ( 1 - 0.05 , len ( x1 ) - 1 , loc = np . mean ( x1 ), scale = stats . sem ( x1 )) Out[26]: (8.461873578892417, 12.058126421107586) In [27]: stats . t . interval ( 1 - 0.05 , len ( x2 ) - 1 , loc = np . mean ( x2 ), scale = stats . sem ( x2 )) Out[27]: (7.663208497074507, 10.376791502925492) In [28]: stats . ttest_ind ( x1 , x2 , equal_var = False ) Out[28]: Ttest_indResult(statistic=1.245268949149111, pvalue=0.23018336828903668) A confidence interval's width is due entirely to sampling error. As the sample size approaches the entire population, the width of the confidence interval approaches zero. Confidence interval has the following general form: $$ \\text{C. I.} = \\text{point estimate} \\pm (\\text{distribution score} \\times \\text{Standard Error}) \\tag{1} $$ where -$D&#94;2$ description description description description description description description description description -$D&#94;2$ description The decision makers always wants to know the uncertainty related to your estimation. This tutorial includes explanation of idea behind C.I., complications that arise when sample data is not normally distributed and solutions for them. For those who don't want to spend more than 2 minutes reading, I included a quick Python code snippets that can generate C.I. for any type of distribution for any type of statistic. Central Limit Theorem may not always apply Nonnormal data, variance known: If the population distribution is not normal and the sample is 'large enough', then X¯ is approximately normal and the same formula provides an approximate 95% CI. The rule that n≥30 is 'large enough' is unreliable here. If the population distribution is heavy-tailed, then X¯ may not have a distribution that is close to normal (even if n≥30). The 'Central Limit Theorem', often provides reasonable approximations for moderate values of n, but it is a limit theorem, with guaranteed results only as n→∞. Many statistical techniques assume that sample data is normally distributed or Gaussian-like (Ex: t-distribution), and different techniques should be considered for non-normal data set. Correct vs. Wrong vs. Useful In [4]: print ( np . mean ( a )) print ( np . std ( a )) 3.375 1.4086784586980805 Extension of Confidence Interval with Bootstrapping In [2]: from scipy.stats import bayes_mvs In [4]: bayes_mvs ([ 1 , 2 , 3 , 5 , 6 , 8 , 5 , 2 ]) Out[4]: (Mean(statistic=4.0, minmax=(2.39878883101482, 5.60121116898518)), Variance(statistic=8.0, minmax=(2.8435061229431486, 18.45571858443238)), Std_dev(statistic=2.691341356821504, minmax=(1.686269884372946, 4.296011939512317))) In [11]: from scipy.stats import truncnorm import matplotlib.pyplot as plt % matplotlib notebook def get_truncated_normal ( mean = 1 , sd = 1 , low =- 3 , upp = 7 ): return truncnorm ( ( low - mean ) / sd , ( upp - mean ) / sd , loc = mean , scale = sd ) X1 = sorted ( get_truncated_normal () . rvs ( 10000 )) mean , std = stats . norm . fit ( X1 , loc = 0 ) pdf_norm = stats . norm . pdf ( X1 , mean , std ) fig , ax = plt . subplots ( figsize = ( 8 , 4 )) ax . plot ( X1 , pdf_norm ) ax . fill_between ( X1 , 0 , pdf_norm , where = ( np . array ( X1 ) >- 1 ) & ( np . array ( X1 ) <= 3 ), facecolor = 'lightgrey' ) # ax.fill_betweenx(pdf_norm, 2, x2=7, interpolate=True) Out[11]: Note: This interval is only exact when the population distribution is normal. For large samples from other population distributions, the interval is approximately correct by the Central Limit Theorem. What is confidence level $1 - \\alpha$? Confidence level is the chance that the statistic value of your interest lies within the interval. A common choice of confidence level include Simulation Ideas https://stats.stackexchange.com/questions/38967/how-robust-is-the-independent-samples-t-test-when-the-distributions-of-the-sampl Practical Application - Bayes Approximation Kruskal-Wallis Hence, in terms of original values, the Kruskal-Wallis is more general than a comparison of means: it tests whether the probability that a random observation from each group is equally likely to be above or below a random observation from another group. The real data quantity that underlies that comparison is neither the differences in means nor the difference in medians, (in the two sample case) it is actually the median of all pairwise differences - the between-sample Hodges-Lehmann difference. Non-Parametric Methods Very robust under normality too. Non-parametric tests (as Wilcoxon-Mann-Whitney) do not rely on a specific distiribution, but implicitly rely on equal variances: under H0 are all samples from the same population. Mann-Whitney https://stats.stackexchange.com/questions/31361/some-questions-about-two-sample-comparisons scipy.stats.mannwhitneyu: It returns a \"One-sided p-value assuming a asymptotic normal distribution\" . Why is it assuming a normal distribution? Should't this test work on any underlying distribution? I think the sentence is referring to the large sample (asymptotic) distribution of the test statistic, not the data. As you can see here, the Mann-Whitney U test statistic has an approximate normal distribution when the sample size is large. There's a difference between Mann-Whiteny, and Wilcoxon Mann-Whitney U test works fine with continuous data, I would even say it works best with them because you would avoid ties. When you have heavier-tailed than normal data, it's also typically more powerful than the t-test The median is usually preferred to other measures of central tendency when your data set is skewed (i.e., forms a skewed distribution) or you are dealing with ordinal data. In no way will the difference in sample sizes adversely affect the Mann-Whitney-Wilcoxon test. Valid only when more than 20 samples FYI, Wikipedia adds that, for large samples, 𝑈 is approximately normally distributed. Given all these values, one can also calculate the effect size η2, https://stats.stackexchange.com/questions/67204/what-exactly-does-a-non-parametric-test-accomplish-what-do-you-do-with-the-res/67210#67210 This link says that Welch t-test should always be used over Mann-Whitney if your goal is to compare central tendency of two distributions: https://stats.stackexchange.com/questions/313471/always-use-welch-t-test-unequal-variances-t-test-instead-of-student-t-or-mann Hotelling's T stats https://courses.lumenlearning.com/boundless-statistics/chapter/the-t-test/ Normality testing take a test on the distribution, e.g. Kolmogorov-Smirnov-test. After that you know whether you have a normal or not. then you need to test neither skewness nor curtosis. The values for asymmetry and kurtosis between -2 and +2 are considered acceptable in order to prove normal univariate distribution (George & Mallery, 2010) F-test For non-normal data, the distribution of the sample variance may deviate substantially from a χ2 distribution. However, if the sample size is large, Slutsky's theorem implies that the distribution of the sample variance has little effect on the distribution of the test statistic. Hypothesis Testing For the unequal variance t test, the null hypothesis is that the two population means are the same but the two population variances may differ. If the P value is large, you don't reject that null hypothesis, so conclude that the evidence does not persuade you that the two population means are different, even though you assume the two populations have (or may have) different standard deviations. What a strange set of assumptions. What would it mean for two populations to have the same mean but different standard deviations? Why would you want to test for that? Swailowsky points out that this situation simply doesn't often come up in science (1). I think the unequal variance t test is more useful when you think about it as a way to create a confidence interval. Your prime goal is not to ask whether two populations differ, but to quantify how far apart the two means are. The unequal variance t test reports a confidence interval for the difference between two means that is usable even if the standard deviations differ. Single sample vs Two samples Two-tailed vs one-tailed For example, we may wish to compare the mean of a sample to a given value x using a t-test. Our null hypothesis is that the mean is equal to x. A two-tailed test will test both if the mean is significantly greater than x and if the mean significantly less than x. The mean is considered significantly different from x if the test statistic is in the top 2.5% or bottom 2.5% of its probability distribution, resulting in a p-value less than 0.05 Our null hypothesis is that the mean is equal to x. A one-tailed test will test either if the mean is significantly greater than x or if the mean is significantly less than x, but not both. Then, depending on the chosen tail, the mean is significantly greater than or less than x if the test statistic is in the top 5% of its probability distribution or bottom 5% of its probability distribution, resulting in a p-value less than 0.05. The one-tailed test provides more power to detect an effect in one direction by not testing the effect in the other direction When is one-tailed appropriate? Imagine you have developed a new drug that you believe is an improvement over an existing drug. You wish to maximize your ability to detect the improvement, so you opt for a one-tailed test. In doing so, you fail to test for the possibility that the new drug is less effective than the existing drug. In testing this drug, you are only interested in testing if it less effective than the existing drug. You do not care if it is significantly more effective. You only wish to show that it is not less effective. In this scenario, a one-tailed test would be appropriate. https://stackoverflow.com/questions/15984221/how-to-perform-two-sample-one-tailed-t-test-with-numpy-scipy Unqual variance t-test https://www.graphpad.com/support/faqid/1568/ https://towardsdatascience.com/kolmogorov-smirnov-test-84c92fb4158d There is an issue with Student's T-Test, samples must be normal (shaped in a normal distribution). That is an issue for us because we do work a lot with Poisson distributions. Really good t-test article https://www.investopedia.com/terms/t/t-test.asp Paired vs unpaired t-test https://www.quora.com/What-is-the-difference-between-a-paired-and-unpaired-t-test Different Sample Sizes https://stats.stackexchange.com/questions/31326/how-should-one-interpret-the-comparison-of-means-from-different-sample-sizes think of this by analogy. If you want to know the area of a rectangle, and the perimeter is fixed, then the area will be maximized if the length and width are equal (i.e., if the rectangle is a square). On the other hand, as the length and width diverge (as the rectangle becomes elongated), the area shrinks. Levene's test It tests the null hypothesis that the population variances are equal (called homogeneity of variance or homoscedasticity) K.S. test My instructor in this topic joked: Kolmogorov-Smirnov is a test for sample size. It has no power in small samples, and intense power, with no particular sensible acceptance of deviations from normal, in large samples. The Kolmogorov-Smirnov (KS) test is used in over 500 refereed papers each year in the astronomical literature. It is a nonparametric hypothesis test that measures the probability that a chosen univariate dataset is drawn from the same parent population as a second dataset (the two-sample KS test) or a continuous model (the one-sample KS test). It measures the greatest distance between the two CDF's. The underlying population distribution is assumed to be continuous. https://asaip.psu.edu/Articles/beware-the-kolmogorov-smirnov-test Question: How good is KS test if sample size is small? even when valid to apply, it is often not very sensitive in establishing distances between two distributions, and a similar EDF-based test gives a better performance. https://stats.stackexchange.com/questions/57885/how-to-interpret-p-value-of-kolmogorov-smirnov-test-python he k-s test returns a D statistic and a p-value corresponding to the D statistic. The D statistic is the absolute max distance (supremum) between the CDFs of the two samples. The closer this number is to 0 the more likely it is that the two samples were drawn from the same distribution. The p-value returned by the k-s test has the same interpretation as other p-values. You reject the null hypothesis that the two samples were drawn from the same distribution if the p-value is less than your significance level. Best when comparing two non-parametric samples But less powerful when comparing to reference distribution, like normal distribution. Shapiro-Wilk is known to be bad with samples with many identical values. Non-normal distribution Mann-Whitney U test, Bootstrapping, lognormal , bayisean CI of medians Normality assumption of a t-test Consider a large population from which you could take many different samples of a particular size. (In a particular study, you generally collect just one of these samples.) By the central limit theorem, means of samples from a population with finite variance approach a normal distribution regardless of the distribution of the population. Rules of thumb say that the sample means are basically normally distributed as long as the sample size is at least 20 or 30. For a t-test to be valid on a sample of smaller size, the population distribution would have to be approximately normal. https://stats.stackexchange.com/questions/9573/t-test-for-non-normal-when-n50 T-test is fine on non-normal data, as long as the deviation from normality isn't large. Visualize your distributions to test this. The advice must be modified somewhat when the distributions are both strongly skewed and very discrete, such as Likert scale items where most of the observations are in one of the end categories. Then the Wilcoxon-Mann-Whitney isn't necessarily a better choice than the t-test. Nonparametric t-Tests The Mann–Whitney U test is the true nonparametric counterpart of the t-test and gives the most accurate estimates of significance, especially when sample sizes are small and/or when the data do not approximate a normal distribution. However, there is something familiar and comforting about using t-tests! When one has a large sample size (N ≫ 30) but the data are skewed, it is worth examining log- or square root-transformed values of the data to see if they become more quasinormal (see Chapter 7). If the data pass a test for normality (included in most statistical software), it is then OK to perform a t-test using the transformed datapoints. When the normality assumption does not hold, a non-parametric alternative to the t-test can often have better statistical power. In the presence of an outlier, the t-test is not robust. For example, for two independent samples when the data distributions are asymmetric (that is, the distributions are skewed) or the distributions have large tails, then the Wilcoxon rank-sum test (also known as the Mann–Whitney U test) can have three to four times higher power than the t-test.[14][15][16] The nonparametric counterpart to the paired samples t-test is the Wilcoxon signed-rank test for paired samples. For a discussion on choosing between the t-test and nonparametric alternatives, see Sawilowsky (2005 https://stats.stackexchange.com/questions/49465/mann-whitney-for-non-normal-distributions-with-n20?rq=1 In Moore, McCabe, Craig's Introduction to the Practice of Statistics (6th ed., pg. 432): For sample sizes 15≤ n ≤39, \"t procedures can be used except in the presence of outliers or strong skewness.\" For samples sizes ≥40, \"t procedures can be used even for clearly skewed distributions.\" The t tends to have reasonably good power relative to the MW for light-tailed distributions ... and can have really bad power for heavy-tailed ones. Skewness tends to be compounded with heavy tails - if power is your main motivation for using the t-test, you should probably avoid it in this case. There's also the possibility of a permutation test rather than either of the choices you mention - it would allow you to test a difference in means and have it be valid when the assumptions of the t-test are not satisfied. Different distribution distances https://statweb.stanford.edu/~souravc/Lecture2.pdf Normality testing in extremely large sample sizes https://stats.stackexchange.com/questions/2492/is-normality-testing-essentially-useless The question normality tests answer: Is there convincing evidence of any deviation from the Gaussian ideal? With moderately large real data sets, the answer is almost always yes. The question scientists often expect the normality test to answer: Do the data deviate enough from the Gaussian ideal to \"forbid\" use of a test that assumes a Gaussian distribution? Scientists often want the normality test to be the referee that decides when to abandon conventional (ANOVA, etc.) tests and instead analyze transformed data or use a rank-based nonparametric test or a resampling or bootstrap approach. For this purpose, normality tests are not very useful. Use skewness or kurtosis as a to test normality instead t can be verified using simulations that this is true for small 𝑛 as well. Thus Student's t-test is sensitive to skewness but relatively robust against heavy tails, and it is reasonable to use a test for normality that is directed towards skew alternatives before applying the t-test. As a rule of thumb (not a law of nature), inference about means is sensitive to skewness and inference about variances is sensitive to kurtosis. - you can use bootstrap in conjuction with normality test to get threshold value of good kurtosis and skewness On large samples, things like the T-test and ANOVA are pretty robust to non-normality. The t-test assumes that the means of the different samples are normally distributed; it does not assume that the population is normally distributed. By the central limit theorem, means of samples from a population with finite variance approach a normal distribution regardless of the distribution of the population. A confidence interval's width is due entirely to sampling error. As the sample size approaches the entire population, the width of the confidence interval approaches zero. Examples Example Suppose a student measuring the boiling temperature of a certain liquid observes the readings (in degrees Celsius) 102.5, 101.7, 103.1, 100.9, 100.5, and 102.2 on 6 different samples of the liquid. He calculates the sample mean to be 101.82. If he knows that the standard deviation for this procedure is 1.2 degrees, what is the confidence interval for the population mean at a 95% confidence level? In other words, the student wishes to estimate the true mean boiling temperature of the liquid using the results of his measurements. If the measurements follow a normal distribution, then the sample mean will have the distribution N(,). Since the sample size is 6, the standard deviation of the sample mean is equal to 1.2/sqrt(6) = 0.49. Outliers T-test is sensitive to outliers. Outliers are important, because they affect the shape of normal distribution. Add one outlier, and run saphiro-wilk normality test, and the result will be significantly different. Using IQR outlier detection might not be good too, because the IQR themselves have outliers in them In [5]: from scipy import stats import numpy as np a = [ 4 , 3 , 6 , 4 , 1 , 2 , 3 , 4 ] stats . norm . interval ( 0.95 , loc = np . mean ( a ), scale = np . std ( a ) / np . sqrt ( len ( a ))) Out[5]: (2.39885356840566, 4.35114643159434) In [7]: import numpy as np , scipy.stats as st st . t . interval ( 0.95 , len ( a ) - 1 , loc = np . mean ( a ), scale = st . sem ( a )) Out[7]: (2.11600213750892, 4.63399786249108) In [ ]: Many statistical techniques assume that sample data is normally distributed or Gaussian - like ( Ex : t - distribution ), and different techniques should be considered for non - normal data set . In [ ]: In [ ]: Similarity of distributions https://stats.stackexchange.com/questions/77888/similarity-between-two-sets-of-random-values Smart Gas Lift gas lift - use external high pressure gas, well has gas in it, but not enough. Inject in through the casing, tubing, and lift hydrocarbons up with the gas. High pressure compression makes the lifespan of machine short. Inject too little gas - don't lift enough. Doesn't flow all the way Inject too much gas - you are gonna lift it all, but you add extra friction, and you waste gas that you can sell. GOR affects how much gas you need to inject. Based on experience, opt injection rate seems to always round about 500, 600, 700 mcfd. Key features self-optimization, maximizing runtime. We have 95% runtime, but others have 60-70% runtime. you can't manually determin BHP, but the algo does it automatically with iterations. Mahalanobis distance https://stats.stackexchange.com/questions/62092/bottom-to-top-explanation-of-the-mahalanobis-distance Distribution score depends on the type of distribution (Ex: normal, lognormal, chi-squared, weibull), and the equations for standard error depends on the type of statistic (Ex: mean, proportion, std, variance). The sample data is assumed to be normally distributed, allowing you to use z-score to lookup values related to any confidence level (Ex: 99%, 95%, 90%). Since you are trying to compute confidence interval of a mean ($\\overline{x}$), you use $SE_{\\overline{x}} = s\\,/\\sqrt{N}$ to compute standard error for a mean.","tags":"Statistics","url":"/understanding_confidence_interval_with_illustrations","loc":"/understanding_confidence_interval_with_illustrations"},{"title":"What is Net Present Value(NPV)?","text":"If you're running a business or working for a company, you'll quickly learn that you need to spend money to make money. There's a balancing act between how much money to spend and how much money you will make. The question is, how do we quantify this so we can easily compare how valueable a potential project is? This is where net present value can be applied. Net present value (NPV) is a commonly used metric in finance and accounting to evaluate and compare anything that involves cash flow, such as loans, investments, and business projects. Net Present Value The benefit of using NPV to evaluate capital projects is that in takes into consideration the time value of money: a dollar today is worth more than a dollar tommorow. Net present value is defined by: $$ NPV = \\sum_{i= 0}&#94;n\\frac{C_i}{(1 + r)&#94;i}\\\\ \\tag{1.1}$$ $\\textbf{where:} \\\\ C_i = \\text{The cash inflow/outflow for period } \\textit{i} \\\\ r = \\text{The discount rate} \\\\ i = \\text{The number of periods} \\\\$ Another way of saying this is: $$ \\\\ \\\\ NPV = \\text{The sum of the present value of all future cash flows} - \\text{The initial investment} \\\\$$ The present value of each cash flow is discounted by discount rate ($r$). This discount rate ($r$) is determined by the minumum return on investment desired. For a project to be considered economic using the NPV method, the NPV must be 0 or greater. Having an NPV of 0 does not mean that the project breaks even, it means that the project earns the minimum required rate of return. A positive NPV means that the project earns more than the required rate of return, or the required rate of return plus the dollar value of NPV. A negative NPV means that the project returns less that the minimum required rate of return, and therefore loses value. $NPV<0$ Project loses value, reject $NPV=0$ Project does not add or lose value, accept if no value adding projects are available $NPV>0$ Project adds value, accept What is the meaning of discount rate? All cash flows are discounted by the discount rate, r, when used in the NPV formula. But what is the meaning of discount rate? The discount rate captures the concept of future dollars not being as valuable as present dollars. Therefore, to determine the present value of all future cash flows, each individual cash flow is devalued as a function of the time from present and the discount rate. For example. If you will recieve $ 1,000 a year from now, and your discount rate is 10% a year, discounted yearly, then its present value is $ 900, or $ 1000 less 10% of $ 1000. If the $ 1000 you are receiving is not coming until 2 years from now, then you must discount it twice, the present value will now be $ 810. How is this discount rate determined? The discount rate is determined by the minumum return on investment that the investor wants. This may be influenced by things such as how the company is being financed or how much the return on investment is on relatively safe projects. The lower you set your discount rate, the more projects will be economical because of the lower required rate of return on investment. Let's see an example of NPV Let's say that you own a factory that makes dog toys. You want to buy a new machine to manufacture toys that costs $ 100,000 to implement, but it will provide cash flows of $ 30,000 per year for 5 years. Your discount rate is 10%. What is the net present value (NPV) of this project and should you implement it? Step 1: Identify all cash flows As we can recall from equation 1 , NPV is a function of cash flows, discount rate, and periods (time). The first thing we must do is identify all cash flows. From the problem, there is a cash flow of $ 30,000 each year, and this project lasts 5 years, so for time periods 1-5 we have a postive cash flow of $ 30,000. We also have to consider that there is a cash flow at time 0, which is the initial investment cost of $ 100,000. Step 2: Evaluate the NPV of the project Let's plug our cash flows into the equation we defined earlier, using a discount rate of 10%: $$ NPV = \\sum_{i= 0}&#94;n\\frac{C_i}{(1 + r)&#94;i} = \\frac{-100,000}{(1 + 0.1)&#94;0} + \\frac{30,000}{(1 + 0.1)&#94;1} + \\frac{30,000}{(1 + 0.1)&#94;2} + \\frac{30,000}{(1 + 0.1)&#94;3} + \\frac{30,000}{(1 + 0.1)&#94;4} + \\frac{30,000}{(1 + 0.1)&#94;5} \\tag{2.1}$$ From here, we can sum up the value of all cash inflow/outflows $$ NPV = \\sum_{i= 0}&#94;n\\frac{C_i}{(1 + r)&#94;i} = -$100,000 + $27,000 + $24,000 + $21,000 + $18,000 + $15,000 = $5,000 \\tag{2.2}$$ Even though the cash inflow each year is $ 30,000, due to the time value of money each cash flow is worth less in future dollars. This is why it's best to earn cash sooner rather than later! Figure 1: $30,000 Yearly cash inflow of project, discounted to present value Our net present value for the project is $ 5,000, which means that this project returns the required 10 percent each year, plus an additional $ 5,000. According for our rules of evalating projects by net present value, we would accept this project. You're happy, and so are the dogs! Net Present Value, or NPV, is an effective way to evaulate captital projects. Because it makes sure you're considering that dollars earned in the future are less valuable than dollars earned now, NPV makes sure you know whether a project is worth your time.","tags":"Finance","url":"/npv-introduction","loc":"/npv-introduction"}]};